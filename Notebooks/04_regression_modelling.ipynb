{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f382c49-686c-423b-8801-663fc0567b2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Regression Modelling\n",
    "\n",
    "## Analytical Assumption\n",
    "\n",
    "We assume that **energy inefficiency is not random** across campus buildings.\n",
    "\n",
    "### Hypothesis\n",
    "> Buildings with certain operational patterns (hour of day, utility type, building size, and readings frequency) can predict `energy_per_sqm`.\n",
    "\n",
    "If this assumption is correct, a regression model should explain a significant portion of energy variation.\n",
    "\n",
    "---\n",
    "\n",
    "## Why Regression?\n",
    "\n",
    "Traditional analysis:\n",
    "- Shows historical inefficiency trends.\n",
    "\n",
    "Regression modelling:\n",
    "- Explains *drivers* of inefficiency.\n",
    "- Predicts future performance.\n",
    "- Supports proactive energy optimization.\n",
    "\n",
    "### Goal\n",
    "Predict **`energy_per_sqm`** and compare model performance against a simple baseline assumption.\n",
    "\n",
    "---\n",
    "\n",
    "## Inputs (from Notebook 03)\n",
    "\n",
    "- `workspace.default.daily_efficiency`\n",
    "- `workspace.default.hourly_efficiency`\n",
    "- `workspace.default.priority_buildings`\n",
    "\n",
    "---\n",
    "\n",
    "## Outputs (from this notebook)\n",
    "\n",
    "- `workspace.default.daily_efficiency_predictions`  \n",
    "  *(Delta table used for dashboarding and advanced analysis)*\n",
    "\n",
    "### Model Evaluation Metrics\n",
    "- RMSE (Root Mean Squared Error)\n",
    "- R² (Coefficient of Determination)\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook performs:\n",
    "\n",
    "- Feature engineering from efficiency tables\n",
    "- Regression model training\n",
    "- Model evaluation\n",
    "- Saving predictions for visualization and dashboards\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33826f75-780c-4c37-936e-56ce280c57e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **Load Tables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3875556b-05ce-45f6-a8d2-20824ecf6500",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "daily_eff = spark.table(\"workspace.default.daily_efficiency\")\n",
    "hourly_eff = spark.table(\"workspace.default.hourly_efficiency\")\n",
    "priority_buildings = spark.table(\"workspace.default.priority_buildings\")\n",
    "\n",
    "display(daily_eff.limit(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d7022825-c377-4c47-82a7-da99234fc597",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f63f0afe-ce59-4954-85b4-bcef0e3d1322",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## # **Assumption Baseline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf0022fa-51f0-46b5-9821-fdb63329562c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "baseline = (\n",
    "    daily_eff\n",
    "    .select(F.avg(\"energy_per_sqm\").alias(\"baseline\"))\n",
    "    .collect()[0][\"baseline\"]\n",
    ")\n",
    "\n",
    "daily_eff = daily_eff.withColumn(\n",
    "    \"baseline_prediction\",\n",
    "    F.lit(baseline)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "75d871cc-41d1-4c16-8a43-68d99d8f4b07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##  Baseline Model\n",
    "Before regression, we assume energy use is constant across campus.\n",
    "\n",
    "This naive assumption becomes our comparison benchmark.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5453eec2-8064-4952-8b98-877813096669",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## FEATURE ENGINEERING (FOR REGRESSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86ca83ac-41a8-4dd5-87aa-2293892b47d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import hour, dayofweek\n",
    "\n",
    "df = (\n",
    "    daily_eff\n",
    "    .withColumn(\"day_of_week\", dayofweek(\"day\"))\n",
    "    .withColumn(\"is_priority\",\n",
    "                F.when(F.col(\"sitename\").isin(\n",
    "                    [r.sitename for r in priority_buildings.select(\"sitename\").collect()]\n",
    "                ), 1).otherwise(0)\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f048859-57fd-48c1-a799-c52428c44424",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## **BUILD REGRESSION MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6dc52293-0ec9-407a-a64e-9a589497e557",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# BUILD REGRESSION MODEL (KEEP METADATA FOR DASHBOARD)\n",
    "# Goal:\n",
    "# Predict energy_per_sqm using engineered efficiency features\n",
    "# AND retain sitename/day/utility for dashboarding.\n",
    "# ============================================================\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# =========================================\n",
    "# 1) LOAD INPUT TABLES\n",
    "# =========================================\n",
    "df = spark.table(\"workspace.default.daily_efficiency\")\n",
    "\n",
    "priority = (\n",
    "    spark.table(\"workspace.default.priority_buildings\")\n",
    "         .select(\"sitename\")\n",
    "         .distinct()\n",
    "         .withColumn(\"is_priority\", F.lit(1))\n",
    ")\n",
    "\n",
    "# =========================================\n",
    "# 2) FEATURE ENGINEERING (CREATE COLUMNS FIRST)\n",
    "# =========================================\n",
    "\n",
    "# 2a) day_of_week from day (0=Mon ... 6=Sun)\n",
    "df = df.withColumn(\"day_of_week\", F.dayofweek(\"day\"))          # 1=Sun ... 7=Sat\n",
    "df = df.withColumn(\"day_of_week\", (F.col(\"day_of_week\") + 5) % 7)  # 0=Mon ... 6=Sun\n",
    "\n",
    "# 2b) is_priority from priority table (left join)\n",
    "df = (\n",
    "    df.join(priority, on=\"sitename\", how=\"left\")\n",
    "      .fillna({\"is_priority\": 0})\n",
    ")\n",
    "\n",
    "# =========================================\n",
    "# 3) CAST FEATURES/LABEL TO NUMERIC\n",
    "# =========================================\n",
    "df = (\n",
    "    df.withColumn(\"square_meters\", F.col(\"square_meters\").cast(\"double\"))\n",
    "      .withColumn(\"n_readings\", F.col(\"n_readings\").cast(\"double\"))\n",
    "      .withColumn(\"day_of_week\", F.col(\"day_of_week\").cast(\"double\"))\n",
    "      .withColumn(\"is_priority\", F.col(\"is_priority\").cast(\"double\"))\n",
    "      .withColumn(\"energy_per_sqm\", F.col(\"energy_per_sqm\").cast(\"double\"))\n",
    ")\n",
    "\n",
    "# =========================================\n",
    "# 4) CLEAN NULLS\n",
    "# =========================================\n",
    "feature_cols = [\"square_meters\", \"n_readings\", \"day_of_week\", \"is_priority\"]\n",
    "df = df.dropna(subset=feature_cols + [\"energy_per_sqm\"])\n",
    "\n",
    "# =========================================\n",
    "# 5) ASSEMBLE FEATURES (KEEP METADATA)\n",
    "# =========================================\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "df_feat = assembler.transform(df)\n",
    "\n",
    "meta_cols = [\"sitename\", \"day\", \"utility\"]\n",
    "\n",
    "model_df = df_feat.select(\n",
    "    *meta_cols,\n",
    "    *feature_cols,\n",
    "    \"features\",\n",
    "    \"energy_per_sqm\"\n",
    ")\n",
    "\n",
    "# =========================================\n",
    "# 6) TRAIN / TEST SPLIT\n",
    "# =========================================\n",
    "train, test = model_df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# =========================================\n",
    "# 7) TRAIN MODEL\n",
    "# =========================================\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"energy_per_sqm\")\n",
    "model = lr.fit(train)\n",
    "\n",
    "# =========================================\n",
    "# 8) PREDICT (METADATA INCLUDED)\n",
    "# =========================================\n",
    "predictions = model.transform(test)\n",
    "\n",
    "display(\n",
    "    predictions.select(\n",
    "        \"sitename\", \"day\", \"utility\",\n",
    "        \"energy_per_sqm\", \"prediction\",\n",
    "        \"square_meters\", \"n_readings\", \"day_of_week\", \"is_priority\"\n",
    "    ).limit(10)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7b532db-07ba-4ea8-9124-a84222737e16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## **EVALUATE MODEL (PROVE ASSUMPTION)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e23b92e0-eedf-497f-a65b-ff95d7498918",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# EVALUATE REGRESSION MODEL (PROVE ASSUMPTION)\n",
    "# Goal:\n",
    "# Measure how well the regression model predicts energy_per_sqm\n",
    "# using standard regression metrics:\n",
    "#   - RMSE (Root Mean Squared Error)\n",
    "#   - R² (Coefficient of Determination)\n",
    "\n",
    "\n",
    "# Import evaluator from Spark ML\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "\n",
    "# STEP 1 — Create RMSE evaluator\n",
    "# RMSE tells us average prediction error magnitude\n",
    "# Lower RMSE = better model\n",
    "\n",
    "rmse_eval = RegressionEvaluator(\n",
    "    labelCol=\"energy_per_sqm\",      # actual values\n",
    "    predictionCol=\"prediction\",     # model output\n",
    "    metricName=\"rmse\"               # evaluation metric\n",
    ")\n",
    "\n",
    "\n",
    "# STEP 2 — Create R² evaluator\n",
    "# R² explains how much variance the model captures\n",
    "# Higher R² = better model\n",
    "\n",
    "r2_eval = RegressionEvaluator(\n",
    "    labelCol=\"energy_per_sqm\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"r2\"\n",
    ")\n",
    "\n",
    "\n",
    "# STEP 3 — Calculate evaluation metrics\n",
    "\n",
    "rmse = rmse_eval.evaluate(predictions)\n",
    "r2 = r2_eval.evaluate(predictions)\n",
    "\n",
    "\n",
    "# STEP 4 — Print model performance results\n",
    "\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"R2:\", r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f997e63-03bf-46f6-b45b-cdb6c5683216",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "## **Interpretation**\n",
    "\n",
    "If R² is high:\n",
    "✔ Our assumption is correct — inefficiency follows predictable patterns.\n",
    "\n",
    "If RMSE is lower than baseline:\n",
    "✔ Regression improves prediction beyond simple averages.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "77da3261-4272-46ef-98ce-9239799bee12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## **Write predictions WITH metadata**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8da3169-9366-477e-8c95-fb11409c1573",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "pred_out = (\n",
    "    predictions\n",
    "      .select(\n",
    "          \"sitename\", \"day\", \"utility\",\n",
    "          \"energy_per_sqm\", \"prediction\",\n",
    "          \"square_meters\", \"n_readings\", \"day_of_week\", \"is_priority\"\n",
    "      )\n",
    "      .withColumn(\"error\", F.col(\"energy_per_sqm\") - F.col(\"prediction\"))\n",
    "      .withColumn(\"abs_error\", F.abs(F.col(\"error\")))\n",
    "      .withColumn(\"squared_error\", F.col(\"error\") * F.col(\"error\"))\n",
    ")\n",
    "\n",
    "(\n",
    "  pred_out.write\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", \"true\")   # key line\n",
    "    .format(\"delta\")\n",
    "    .saveAsTable(\"workspace.default.daily_efficiency_predictions\")\n",
    ")\n",
    "\n",
    "display(pred_out.limit(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f436658-2df8-4fab-bd6a-0fb70b783103",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## **SAVE OUTPUT FOR DASHBOARD**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "138adbb2-2c71-4216-9924-052146021008",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(predictions.write\n",
    " .mode(\"overwrite\")\n",
    " .format(\"delta\")\n",
    " .saveAsTable(\"workspace.default.daily_efficiency_predictions\"))\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04_regression_modelling",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
